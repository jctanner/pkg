#!/usr/bin/env python3

import os
import sys
import yaml
import shutil
import urllib.request
import subprocess
import tempfile
import zipfile
import tarfile
import hashlib
from pathlib import Path
from datetime import datetime, timezone
import argparse
from urllib.parse import urlparse

try:
    from packaging.version import parse as parse_version
    from jinja2 import Template
except ImportError:
    print("Missing dependencies: Run 'pip install packaging jinja2 pyyaml'")
    sys.exit(1)

DEFAULT_INSTALL_DB = os.path.expanduser("~/.local/share/pkg")
DEFAULT_RECIPE_REPO = "https://github.com/jctanner/pkg"


def expand(path):
    return str(Path(os.path.expandvars(os.path.expanduser(path))))


def get_install_db_path(cli_override=None):
    return expand(cli_override or os.environ.get("PKG_DB") or DEFAULT_INSTALL_DB)


def get_recipe_repo(cli_override=None):
    return cli_override or os.environ.get("PKG_RECIPE_REPO") or DEFAULT_RECIPE_REPO


def get_raw_recipe_url(recipe_repo, name):
    # Convert GitHub repo URL to raw URL if it's github.com based
    if recipe_repo.startswith("https://github.com/"):
        parts = recipe_repo.rstrip("/").split("/")
        if len(parts) >= 5:
            user, repo = parts[3], parts[4]
            return f"https://raw.githubusercontent.com/{user}/{repo}/main/recipes/{name}.yaml"
        elif len(parts) == 3:
            user, repo = parts[3], parts[4]
            return f"https://raw.githubusercontent.com/{user}/{repo}/main/recipes/{name}.yaml"
    return f"{recipe_repo.rstrip('/')}/{name}.yaml"


def evaluate_template(template_str, context):
    return Template(template_str).render(**context)


def collect_variables(spec):
    context = {}
    for key, val in spec.get("variables", {}).items():
        if isinstance(val, dict):
            if "shell" in val:
                result = subprocess.run(
                    val["shell"],
                    shell=True,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                )
                context[key] = result.stdout.decode("utf-8").strip()
            elif "literal" in val:
                context[key] = val["literal"]
            else:
                raise ValueError(f"Variable '{key}' must define 'shell' or 'literal'")
        else:
            context[key] = str(val)
    return context


def run_build_steps(steps, cwd, env=None):
    for step in steps:
        print(f"Running: {step}")
        result = subprocess.run(step, shell=True, cwd=cwd, env=env)
        if result.returncode != 0:
            raise RuntimeError(f"Build step failed: {step}")


def download(url, dest):
    print(f"Downloading: {url}")
    with urllib.request.urlopen(url) as response:
        with open(dest, "wb") as out_file:
            out_file.write(response.read())


def calculate_sha256(filepath):
    """Calculate SHA256 checksum of a file."""
    sha256_hash = hashlib.sha256()
    try:
        with open(filepath, "rb") as f:
            # Read file in chunks to handle large files
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        print(f"Warning: Failed to calculate checksum for {filepath}: {e}")
        return None


def apply_chmod_recursive(path, chmod_spec, install_mode):
    """Apply chmod permissions to files, with special handling for directory mode."""
    if not chmod_spec:
        return

    try:
        # Parse chmod specification
        if isinstance(chmod_spec, int) or str(chmod_spec).isdigit():
            mode = int(str(chmod_spec), 8)
        elif chmod_spec.startswith("0o"):
            mode = int(chmod_spec, 8)
        elif chmod_spec.startswith("+"):
            st = os.stat(path)
            mode = st.st_mode | int(chmod_spec.replace("+", "0o"), 8)
        else:
            raise ValueError("Unsupported chmod format")

        if install_mode == 'single' or not os.path.exists(path):
            # Single file mode or file doesn't exist
            if os.path.exists(path):
                os.chmod(path, mode)
                print(f"Set permissions: {oct(mode)} on {path}")
        elif os.path.isfile(path):
            # Single file in directory mode
            if os.access(path, os.X_OK):  # Only chmod if already executable
                os.chmod(path, mode)
                print(f"Set permissions: {oct(mode)} on {path}")
        elif os.path.isdir(path):
            # Directory mode: apply to executables only
            for root, dirs, files in os.walk(path):
                for fname in files:
                    fpath = os.path.join(root, fname)
                    # Only chmod files that are already executable
                    if os.access(fpath, os.X_OK):
                        os.chmod(fpath, mode)
                        print(f"Set permissions: {oct(mode)} on {fpath}")
    except Exception as e:
        print(f"Failed to apply chmod '{chmod_spec}' to {path}: {e}")


def extract_archive(archive_type, archive_path, extract_list, dest_dir, install_mode='single'):
    """
    Extract files/directories from archive.

    Args:
        archive_type: 'tar' or 'zip'
        archive_path: Path to archive file
        extract_list: List of files/directories to extract
        dest_dir: Temporary extraction directory
        install_mode: 'single' (default) or 'directory'

    Returns:
        List of extracted paths (files or directories) in dest_dir
    """
    extracted_paths = []

    if archive_type == "tar":
        with tarfile.open(archive_path, "r:*") as tar:
            members = tar.getmembers()

            for pattern in extract_list:
                if install_mode == 'single':
                    # Legacy behavior: match by basename for single file
                    matched = [m for m in members if m.name == pattern or os.path.basename(m.name) == pattern]
                    if not matched:
                        raise FileNotFoundError(f"{pattern} not found in archive")
                    for m in matched:
                        tar.extract(m, path=dest_dir)
                    # Return the first matched file path for backward compatibility
                    extracted_paths.append(os.path.join(dest_dir, matched[0].name))

                elif install_mode == 'directory':
                    # New behavior: support directory extraction
                    if pattern.endswith('/'):
                        # Extract directory contents (flatten)
                        dir_prefix = pattern.rstrip('/')
                        dir_members = [m for m in members
                                     if m.name.startswith(dir_prefix + '/') and m.name != dir_prefix]
                        if not dir_members:
                            raise FileNotFoundError(f"Directory {pattern} not found or empty in archive")

                        # Extract all files from the directory
                        for m in dir_members:
                            tar.extract(m, path=dest_dir)
                            # Calculate relative path within the directory
                            rel_path = m.name[len(dir_prefix)+1:]  # Remove "go/bin/" prefix
                            if rel_path and not m.isdir():  # Skip the directory itself and empty paths
                                extracted_paths.append(os.path.join(dest_dir, m.name))
                    else:
                        # Extract directory with structure preserved
                        matched = [m for m in members if m.name == pattern or m.name.startswith(pattern + '/')]
                        if not matched:
                            raise FileNotFoundError(f"{pattern} not found in archive")
                        for m in matched:
                            tar.extract(m, path=dest_dir)
                        extracted_paths.append(os.path.join(dest_dir, pattern))

    elif archive_type == "zip":
        with zipfile.ZipFile(archive_path, "r") as zipf:
            namelist = zipf.namelist()

            for pattern in extract_list:
                if install_mode == 'single':
                    # Legacy behavior
                    matched = [z for z in namelist if os.path.basename(z) == pattern]
                    if not matched:
                        raise FileNotFoundError(f"{pattern} not found in archive")
                    for z in matched:
                        zipf.extract(z, path=dest_dir)
                    extracted_paths.append(os.path.join(dest_dir, matched[0]))

                elif install_mode == 'directory':
                    if pattern.endswith('/'):
                        # Extract directory contents (flatten)
                        dir_prefix = pattern.rstrip('/')
                        dir_files = [z for z in namelist
                                   if z.startswith(dir_prefix + '/') and z != dir_prefix and z != dir_prefix + '/']
                        if not dir_files:
                            raise FileNotFoundError(f"Directory {pattern} not found or empty in archive")

                        for z in dir_files:
                            zipf.extract(z, path=dest_dir)
                            rel_path = z[len(dir_prefix)+1:]
                            if rel_path and not z.endswith('/'):  # Skip directory entries
                                extracted_paths.append(os.path.join(dest_dir, z))
                    else:
                        matched = [z for z in namelist if z == pattern or z.startswith(pattern + '/')]
                        if not matched:
                            raise FileNotFoundError(f"{pattern} not found in archive")
                        for z in matched:
                            zipf.extract(z, path=dest_dir)
                        extracted_paths.append(os.path.join(dest_dir, pattern))
    else:
        raise ValueError(f"Unsupported archive type: {archive_type}")

    return extracted_paths


def resolve_yaml_source(yaml_path, recipe_repo=None):
    parsed = urlparse(yaml_path)
    if parsed.scheme in ("http", "https"):
        print(f"Fetching remote YAML: {yaml_path}")
        tmpfd, tmpfile = tempfile.mkstemp(suffix=".yaml")
        os.close(tmpfd)
        with urllib.request.urlopen(yaml_path) as response:
            with open(tmpfile, "wb") as out:
                out.write(response.read())
        return tmpfile, yaml_path

    if os.path.isfile(yaml_path):
        return yaml_path, os.path.abspath(yaml_path)

    if not recipe_repo:
        recipe_repo = get_recipe_repo()

    if recipe_repo:
        remote_url = get_raw_recipe_url(recipe_repo, yaml_path)
        print(f"Attempting to fetch recipe '{yaml_path}' from {remote_url}")
        tmpfd, tmpfile = tempfile.mkstemp(suffix=".yaml")
        os.close(tmpfd)
        try:
            with urllib.request.urlopen(remote_url) as response:
                with open(tmpfile, "wb") as out:
                    out.write(response.read())
            return tmpfile, remote_url
        except Exception as e:
            print(f"Failed to fetch recipe from remote: {e}")
            sys.exit(1)

    raise FileNotFoundError(
        f"YAML path not found and no valid recipe repo fallback: {yaml_path}"
    )


def install_tool(
    yaml_file, override_dest_dir=None, install_db_dir=None, recipe_repo=None
):
    local_yaml_file, source_yaml = resolve_yaml_source(
        yaml_file, recipe_repo=recipe_repo
    )

    with open(local_yaml_file) as f:
        spec = yaml.safe_load(f)

    name = spec["name"]
    version = spec.get("version", "unknown")
    chmod_spec = spec.get("chmod")
    install_mode = spec.get("install_mode", "single")  # NEW: Get install mode
    install_name = spec.get("install_name")  # NEW: Optional rename for installed directory
    symlink_bin = spec.get("symlink_bin")  # NEW: Path to create symlinks for bin/ executables
    source_yaml = os.path.abspath(yaml_file)
    context = collect_variables(spec)

    # Override top-level version with variable if available
    if "version" in context:
        version = context["version"]
    else:
        context["version"] = version

    url = evaluate_template(spec["url"], context) if "url" in spec else None
    dest_path = expand(evaluate_template(spec["dest"], context))

    # NEW: Evaluate install_name and symlink_bin with templating
    if install_name:
        install_name = evaluate_template(install_name, context)
    if symlink_bin:
        symlink_bin = expand(evaluate_template(symlink_bin, context))

    # NEW: Handle dest differently based on install_mode
    installed_files = []  # Track all installed files

    if install_mode == 'single':
        # Legacy behavior: dest is the full path to the file
        if override_dest_dir:
            override_dest_dir = expand(override_dest_dir)
            os.makedirs(override_dest_dir, exist_ok=True)
            final_dest = os.path.join(override_dest_dir, os.path.basename(dest_path))
        else:
            os.makedirs(os.path.dirname(dest_path), exist_ok=True)
            final_dest = dest_path
    elif install_mode == 'directory':
        # New behavior: dest is the parent directory for multiple files
        if override_dest_dir:
            final_dest = expand(override_dest_dir)
        else:
            final_dest = dest_path
        os.makedirs(final_dest, exist_ok=True)
    else:
        raise ValueError(f"Invalid install_mode: {install_mode}. Must be 'single' or 'directory'")

    if url:
        archive = spec.get("archive")
        extract = spec.get("extract")
        if extract is not None:
            extract = [evaluate_template(x, context) for x in extract]
        if archive and extract:
            with tempfile.TemporaryDirectory(prefix=f"pkg-{name}-") as tmpdir:
                tmp_archive = os.path.join(tmpdir, "archive")
                download(url, tmp_archive)
                # NEW: Pass install_mode and get list of extracted paths
                extracted_paths = extract_archive(archive, tmp_archive, extract, tmpdir, install_mode)

                if install_mode == 'single':
                    # Legacy: Copy single file
                    shutil.copy(extracted_paths[0], final_dest)
                    installed_files.append(final_dest)
                elif install_mode == 'directory':
                    # New: Copy each extracted file/directory to dest directory
                    for extracted_path in extracted_paths:
                        # Get the basename of the file or directory
                        basename = os.path.basename(extracted_path)

                        # NEW: Use install_name to rename if specified
                        if install_name and os.path.isdir(extracted_path):
                            dest_path = os.path.join(final_dest, install_name)
                        else:
                            dest_path = os.path.join(final_dest, basename)

                        if os.path.isdir(extracted_path):
                            # Copy entire directory tree
                            if os.path.exists(dest_path):
                                shutil.rmtree(dest_path)
                            shutil.copytree(extracted_path, dest_path)
                            installed_files.append(dest_path)
                            if install_name and install_name != basename:
                                print(f"Copied directory {basename} to {dest_path} (renamed to {install_name})")
                            else:
                                print(f"Copied directory {basename} to {dest_path}")
                        else:
                            # Copy single file
                            shutil.copy(extracted_path, dest_path)
                            installed_files.append(dest_path)
                            print(f"Copied {basename} to {dest_path}")
        else:
            # Direct download (no archive)
            download(url, final_dest)
            installed_files.append(final_dest)

    elif "git" in spec:
        git_url = evaluate_template(spec["git"], context)
        build_steps = spec.get("build", [])
        artifact = evaluate_template(spec.get("artifact", ""), context)
        if not artifact:
            raise ValueError("Missing 'artifact' field for git build")
        with tempfile.TemporaryDirectory(prefix=f"pkg-{name}-") as tmpdir:
            print(f"Cloning {git_url} into {tmpdir}")
            subprocess.run(["git", "clone", "--depth=1", git_url, tmpdir], check=True)
            run_build_steps(build_steps, cwd=tmpdir)
            artifact_path = os.path.join(tmpdir, artifact)
            if not os.path.exists(artifact_path):
                raise FileNotFoundError(f"Built artifact not found: {artifact_path}")
            # Git builds currently only support single file mode
            if install_mode == 'single':
                shutil.copy(artifact_path, final_dest)
                installed_files.append(final_dest)
            else:
                raise ValueError("Git builds currently only support install_mode='single'")

    else:
        raise ValueError("YAML must contain either 'url' or 'git' field")

    # NEW: Apply chmod using new helper function
    if install_mode == 'single':
        apply_chmod_recursive(final_dest, chmod_spec, install_mode)
    else:
        # For directory mode, apply chmod to each installed file
        for installed_file in installed_files:
            apply_chmod_recursive(installed_file, chmod_spec, install_mode)

    # NEW: Create symlinks for bin executables if requested
    created_symlinks = []
    if symlink_bin and install_mode == 'directory':
        os.makedirs(symlink_bin, exist_ok=True)
        # Find all executables in installed_files/bin/ directory
        for installed_path in installed_files:
            bin_dir = os.path.join(installed_path, 'bin')
            if os.path.isdir(bin_dir):
                for fname in os.listdir(bin_dir):
                    fpath = os.path.join(bin_dir, fname)
                    if os.path.isfile(fpath) and os.access(fpath, os.X_OK):
                        # Create symlink in symlink_bin directory
                        symlink_path = os.path.join(symlink_bin, fname)
                        if os.path.lexists(symlink_path):
                            os.remove(symlink_path)  # Remove existing symlink/file
                        os.symlink(fpath, symlink_path)
                        created_symlinks.append(symlink_path)
                        print(f"Created symlink: {symlink_path} -> {fpath}")

    # Print installation summary
    if install_mode == 'single':
        print(f"Installed {name} to {final_dest}")
    else:
        print(f"Installed {name} to {final_dest} ({len(installed_files)} files)")
        for f in installed_files:
            print(f"  - {f}")
        if created_symlinks:
            print(f"Created {len(created_symlinks)} symlink(s) in {symlink_bin}")

    install_db_dir = get_install_db_path(install_db_dir)
    os.makedirs(install_db_dir, exist_ok=True)

    hash_input = f"{name}|{version}|{final_dest}|{source_yaml}"
    install_id = hashlib.sha256(hash_input.encode()).hexdigest()[:12]
    filename = f"PKG_{install_id}.yaml"

    # NEW: Calculate SHA256 checksums for all installed files
    checksums = {}
    all_files = []  # Track all individual files for checksums
    for filepath in installed_files:
        if os.path.isfile(filepath):
            checksum = calculate_sha256(filepath)
            if checksum:
                checksums[filepath] = checksum
                all_files.append(filepath)
        elif os.path.isdir(filepath):
            # For directories, calculate checksums for all files within
            for root, dirs, files in os.walk(filepath):
                for fname in files:
                    fpath = os.path.join(root, fname)
                    checksum = calculate_sha256(fpath)
                    if checksum:
                        checksums[fpath] = checksum
                        all_files.append(fpath)

    # NEW: Enhanced metadata format with install_mode and installed_files
    rendered_metadata = {
        "name": name,
        "version": version,
        "url": url,
        "dest": dest_path,
        "installed_to": final_dest,
        "installed_at": datetime.now(timezone.utc).isoformat(),
        "variables": context,
        "install_mode": install_mode,  # NEW: Track install mode
        "installed_files": installed_files,  # NEW: Track all installed files
        "checksums": checksums,  # NEW: Track SHA256 checksums
        "symlinks": created_symlinks,  # NEW: Track created symlinks
    }

    with open(os.path.join(install_db_dir, filename), "w") as f:
        yaml.safe_dump_all([spec, rendered_metadata], f, sort_keys=False)


def list_installed(install_db_dir=None):
    install_db_dir = get_install_db_path(install_db_dir)
    if not os.path.isdir(install_db_dir):
        print("No tools installed.")
        return

    # NEW: Updated header with MODE and INSTALLED columns
    print(f"{'ID':12} {'NAME':20} {'VERSION':10} {'MODE':8} {'INSTALLED':16} {'PATH'}")
    for fname in sorted(os.listdir(install_db_dir)):
        if not fname.startswith("PKG_") or not fname.endswith(".yaml"):
            continue
        install_id = fname[4:-5]
        with open(os.path.join(install_db_dir, fname)) as f:
            # Optimization: Skip loading the entire file with checksums
            # Read file content and split on document separator
            content = f.read()
            # Find the second YAML document (metadata)
            parts = content.split('\n---\n', 1)
            if len(parts) < 2:
                continue
            # Parse only the metadata portion, skip checksums to avoid loading 14k+ entries
            metadata_yaml = parts[1]
            # Find where checksums section starts and truncate there
            checksum_pos = metadata_yaml.find('\nchecksums:')
            if checksum_pos != -1:
                # Only parse metadata up to checksums
                metadata_yaml = metadata_yaml[:checksum_pos]
            meta = yaml.safe_load(metadata_yaml) or {}

            # NEW: Get install_mode and file count
            install_mode = meta.get('install_mode', 'single')
            mode_display = install_mode[:6]  # Truncate for display
            installed_path = meta.get('installed_to', '')

            # NEW: Format installed_at timestamp
            installed_at = meta.get('installed_at', '')
            if installed_at:
                try:
                    # Parse ISO format timestamp and show just date and time
                    dt = datetime.fromisoformat(installed_at.replace('Z', '+00:00'))
                    date_display = dt.strftime('%Y-%m-%dT%H:%M')
                except:
                    date_display = installed_at[:16]  # Fallback to first 16 chars
            else:
                date_display = 'unknown'

            # NEW: Show count for multi-file installations
            if install_mode != 'single' and 'installed_files' in meta:
                file_count = len(meta['installed_files'])
                installed_path = f"{installed_path} ({file_count} files)"

            print(
                f"{install_id:12} {meta.get('name',''):20} {meta.get('version',''):10} {mode_display:8} {date_display:16} {installed_path}"
            )


def uninstall_tool(install_id, install_db_dir=None):
    install_db_dir = get_install_db_path(install_db_dir)
    fname = f"PKG_{install_id}.yaml"
    path = os.path.join(install_db_dir, fname)
    if not os.path.exists(path):
        print(f"No metadata found for install ID {install_id}")
        return

    with open(path) as f:
        docs = list(yaml.safe_load_all(f))
        meta = docs[1] if len(docs) > 1 else {}

    # NEW: Get install_mode and installed_files with backward compatibility
    install_mode = meta.get("install_mode", "single")
    installed_files = meta.get("installed_files", [])
    symlinks = meta.get("symlinks", [])

    # Backward compatibility: fall back to installed_to if no installed_files
    if not installed_files:
        installed_to = meta.get("installed_to")
        if installed_to:
            installed_files = [installed_to]

    # NEW: Remove symlinks first
    if symlinks:
        for symlink in symlinks:
            if os.path.lexists(symlink):
                print(f"Removing symlink: {symlink}")
                try:
                    os.remove(symlink)
                except Exception as e:
                    print(f"Failed to remove symlink {symlink}: {e}")

    # Remove all installed files/directories
    removed_count = 0
    for item in installed_files:
        if not item:
            continue

        if os.path.isdir(item):
            # Remove directory tree
            print(f"Removing directory: {item}")
            try:
                shutil.rmtree(item)
                removed_count += 1
            except Exception as e:
                print(f"Failed to remove directory {item}: {e}")
        elif os.path.isfile(item):
            # Remove single file
            print(f"Removing file: {item}")
            try:
                os.remove(item)
                removed_count += 1
            except Exception as e:
                print(f"Failed to remove file {item}: {e}")
        else:
            print(f"Path does not exist (may have been manually deleted): {item}")

    if removed_count > 0:
        print(f"Removed {removed_count} file(s)/directory(ies)")

    print(f"Removing metadata: {path}")
    os.remove(path)


def verify_tool(install_id, install_db_dir=None):
    """Verify that installed files exist and match their stored checksums."""
    install_db_dir = get_install_db_path(install_db_dir)
    fname = f"PKG_{install_id}.yaml"
    path = os.path.join(install_db_dir, fname)

    if not os.path.exists(path):
        print(f"No metadata found for install ID {install_id}")
        return False

    with open(path) as f:
        docs = list(yaml.safe_load_all(f))
        meta = docs[1] if len(docs) > 1 else {}

    name = meta.get("name", "unknown")
    version = meta.get("version", "unknown")
    install_mode = meta.get("install_mode", "single")
    installed_files = meta.get("installed_files", [])
    stored_checksums = meta.get("checksums", {})

    # Backward compatibility: fall back to installed_to if no installed_files
    if not installed_files:
        installed_to = meta.get("installed_to")
        if installed_to:
            installed_files = [installed_to]

    if not installed_files:
        print(f"✗ {name} {version}: No files tracked in metadata")
        return False

    print(f"Verifying {name} {version} ({install_mode} mode):")

    all_valid = True

    # If we have checksums, verify all files with checksums
    if stored_checksums:
        valid_count = 0
        invalid_count = 0
        missing_count = 0

        for filepath, expected_checksum in stored_checksums.items():
            if not os.path.exists(filepath):
                print(f"  ✗ {filepath}: File missing")
                missing_count += 1
                all_valid = False
            else:
                actual_checksum = calculate_sha256(filepath)
                if actual_checksum == expected_checksum:
                    valid_count += 1
                else:
                    print(f"  ✗ {filepath}: Checksum mismatch")
                    print(f"    Expected: {expected_checksum}")
                    print(f"    Actual:   {actual_checksum}")
                    invalid_count += 1
                    all_valid = False

        # Show summary
        total = len(stored_checksums)
        print(f"  Summary: {valid_count}/{total} files valid", end="")
        if invalid_count > 0:
            print(f", {invalid_count} checksum mismatches", end="")
        if missing_count > 0:
            print(f", {missing_count} missing", end="")
        print()
    else:
        # No checksums stored (old metadata format) - just check existence
        for filepath in installed_files:
            if not os.path.exists(filepath):
                print(f"  ✗ {filepath}: Path missing")
                all_valid = False
            elif os.path.isfile(filepath):
                print(f"  ? {filepath}: Exists (no checksum to verify)")
            elif os.path.isdir(filepath):
                print(f"  ? {filepath}: Directory exists (no checksum)")

    return all_valid


def verify_all_tools(install_db_dir=None):
    """Verify all installed packages."""
    install_db_dir = get_install_db_path(install_db_dir)
    if not os.path.isdir(install_db_dir):
        print("No tools installed.")
        return True

    all_packages_valid = True
    package_count = 0
    valid_count = 0
    invalid_count = 0

    for fname in sorted(os.listdir(install_db_dir)):
        if not fname.startswith("PKG_") or not fname.endswith(".yaml"):
            continue

        install_id = fname[4:-5]
        package_count += 1

        # Verify this package
        is_valid = verify_tool(install_id, install_db_dir)
        if is_valid:
            valid_count += 1
        else:
            invalid_count += 1
            all_packages_valid = False

        print()  # Blank line between packages

    # Print summary
    print("=" * 60)
    print(f"Verification Summary:")
    print(f"  Total packages: {package_count}")
    print(f"  Valid: {valid_count}")
    print(f"  Invalid: {invalid_count}")

    return all_packages_valid


def main():
    parser = argparse.ArgumentParser(description="Minimal YAML-based installer (pkg)")
    subparsers = parser.add_subparsers(dest="command", required=True)

    def add_common_args(p):
        p.add_argument("--dest-dir", help="Override destination directory (e.g. ~/bin)")
        p.add_argument("--install-db", help="Override install DB path (or use $PKG_DB)")
        p.add_argument(
            "--recipe-repo", help="Recipe repo (e.g. GitHub URL or local path)"
        )
        p.add_argument("yaml_file", help="YAML file, URL, or recipe name")

    p_install = subparsers.add_parser("install", help="Install a tool from YAML")
    add_common_args(p_install)

    p_list = subparsers.add_parser("list", help="List installed tools")
    p_list.add_argument("--install-db", help="Override install DB path")

    p_uninstall = subparsers.add_parser(
        "uninstall", help="Uninstall a tool by install ID"
    )
    p_uninstall.add_argument("install_id", help="Install ID shown by 'pkg list'")
    p_uninstall.add_argument("--install-db", help="Override install DB path")

    p_verify = subparsers.add_parser(
        "verify", help="Verify installed files match their checksums"
    )
    p_verify.add_argument("install_id", nargs='?', help="Install ID shown by 'pkg list' (omit to verify all)")
    p_verify.add_argument("--install-db", help="Override install DB path")
    p_verify.add_argument("--all", action="store_true", help="Verify all installed packages")

    args = parser.parse_args()

    if args.command == "install":
        install_tool(
            args.yaml_file,
            override_dest_dir=args.dest_dir,
            install_db_dir=args.install_db,
            recipe_repo=args.recipe_repo,
        )
    elif args.command == "list":
        list_installed(args.install_db)
    elif args.command == "uninstall":
        uninstall_tool(args.install_id, args.install_db)
    elif args.command == "verify":
        if args.all or args.install_id is None:
            verify_all_tools(args.install_db)
        else:
            verify_tool(args.install_id, args.install_db)


if __name__ == "__main__":
    main()
