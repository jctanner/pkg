#!/usr/bin/env python3

import os
import sys
import yaml
import shutil
import urllib.request
import subprocess
import tempfile
import zipfile
import tarfile
import hashlib
from pathlib import Path
from datetime import datetime, timezone
import argparse
from urllib.parse import urlparse

try:
    from packaging.version import parse as parse_version
    from jinja2 import Template
except ImportError:
    print("Missing dependencies: Run 'pip install packaging jinja2 pyyaml'")
    sys.exit(1)

DEFAULT_INSTALL_DB = os.path.expanduser("~/.local/share/pkg")
DEFAULT_RECIPE_REPO = "https://github.com/jctanner/pkg"


def expand(path):
    return str(Path(os.path.expandvars(os.path.expanduser(path))))


def get_install_db_path(cli_override=None):
    return expand(cli_override or os.environ.get("PKG_DB") or DEFAULT_INSTALL_DB)


def get_recipe_repo(cli_override=None):
    return cli_override or os.environ.get("PKG_RECIPE_REPO") or DEFAULT_RECIPE_REPO


def get_raw_recipe_url(recipe_repo, name):
    # Convert GitHub repo URL to raw URL if it's github.com based
    if recipe_repo.startswith("https://github.com/"):
        parts = recipe_repo.rstrip("/").split("/")
        if len(parts) >= 5:
            user, repo = parts[3], parts[4]
            return f"https://raw.githubusercontent.com/{user}/{repo}/main/recipes/{name}.yaml"
        elif len(parts) == 3:
            user, repo = parts[3], parts[4]
            return f"https://raw.githubusercontent.com/{user}/{repo}/main/recipes/{name}.yaml"
    return f"{recipe_repo.rstrip('/')}/{name}.yaml"


def evaluate_template(template_str, context):
    return Template(template_str).render(**context)


def collect_variables(spec):
    context = {}
    for key, val in spec.get("variables", {}).items():
        if isinstance(val, dict):
            if "shell" in val:
                result = subprocess.run(
                    val["shell"],
                    shell=True,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                )
                context[key] = result.stdout.decode("utf-8").strip()
            elif "literal" in val:
                context[key] = val["literal"]
            else:
                raise ValueError(f"Variable '{key}' must define 'shell' or 'literal'")
        else:
            context[key] = str(val)
    return context


def run_build_steps(steps, cwd, env=None):
    for step in steps:
        print(f"Running: {step}")
        result = subprocess.run(step, shell=True, cwd=cwd, env=env)
        if result.returncode != 0:
            raise RuntimeError(f"Build step failed: {step}")


def download(url, dest):
    print(f"Downloading: {url}")
    with urllib.request.urlopen(url) as response:
        with open(dest, "wb") as out_file:
            out_file.write(response.read())


def calculate_sha256(filepath):
    """Calculate SHA256 checksum of a file."""
    sha256_hash = hashlib.sha256()
    try:
        with open(filepath, "rb") as f:
            # Read file in chunks to handle large files
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        print(f"Warning: Failed to calculate checksum for {filepath}: {e}")
        return None


def apply_chmod_recursive(path, chmod_spec, install_mode):
    """Apply chmod permissions to files, with special handling for directory mode."""
    if not chmod_spec:
        return

    try:
        # Parse chmod specification
        if isinstance(chmod_spec, int) or str(chmod_spec).isdigit():
            mode = int(str(chmod_spec), 8)
        elif chmod_spec.startswith("0o"):
            mode = int(chmod_spec, 8)
        elif chmod_spec.startswith("+"):
            st = os.stat(path)
            mode = st.st_mode | int(chmod_spec.replace("+", "0o"), 8)
        else:
            raise ValueError("Unsupported chmod format")

        if install_mode == 'single' or not os.path.exists(path):
            # Single file mode or file doesn't exist
            if os.path.exists(path):
                os.chmod(path, mode)
                print(f"Set permissions: {oct(mode)} on {path}")
        elif os.path.isfile(path):
            # Single file in directory mode
            if os.access(path, os.X_OK):  # Only chmod if already executable
                os.chmod(path, mode)
                print(f"Set permissions: {oct(mode)} on {path}")
        elif os.path.isdir(path):
            # Directory mode: apply to executables only
            for root, dirs, files in os.walk(path):
                for fname in files:
                    fpath = os.path.join(root, fname)
                    # Only chmod files that are already executable
                    if os.access(fpath, os.X_OK):
                        os.chmod(fpath, mode)
                        print(f"Set permissions: {oct(mode)} on {fpath}")
    except Exception as e:
        print(f"Failed to apply chmod '{chmod_spec}' to {path}: {e}")


def extract_archive(archive_type, archive_path, extract_list, dest_dir, install_mode='single'):
    """
    Extract files/directories from archive.

    Args:
        archive_type: 'tar' or 'zip'
        archive_path: Path to archive file
        extract_list: List of files/directories to extract
        dest_dir: Temporary extraction directory
        install_mode: 'single' (default) or 'directory'

    Returns:
        List of extracted paths (files or directories) in dest_dir
    """
    extracted_paths = []

    if archive_type == "tar":
        with tarfile.open(archive_path, "r:*") as tar:
            members = tar.getmembers()

            for pattern in extract_list:
                if install_mode == 'single':
                    # Legacy behavior: match by basename for single file
                    matched = [m for m in members if m.name == pattern or os.path.basename(m.name) == pattern]
                    if not matched:
                        raise FileNotFoundError(f"{pattern} not found in archive")
                    for m in matched:
                        tar.extract(m, path=dest_dir)
                    # Return the first matched file path for backward compatibility
                    extracted_paths.append(os.path.join(dest_dir, matched[0].name))

                elif install_mode == 'directory':
                    # New behavior: support directory extraction
                    if pattern.endswith('/'):
                        # Extract directory contents (flatten)
                        dir_prefix = pattern.rstrip('/')
                        dir_members = [m for m in members
                                     if m.name.startswith(dir_prefix + '/') and m.name != dir_prefix]
                        if not dir_members:
                            raise FileNotFoundError(f"Directory {pattern} not found or empty in archive")

                        # Extract all files from the directory
                        for m in dir_members:
                            tar.extract(m, path=dest_dir)
                            # Calculate relative path within the directory
                            rel_path = m.name[len(dir_prefix)+1:]  # Remove "go/bin/" prefix
                            if rel_path and not m.isdir():  # Skip the directory itself and empty paths
                                extracted_paths.append(os.path.join(dest_dir, m.name))
                    else:
                        # Extract directory with structure preserved
                        matched = [m for m in members if m.name == pattern or m.name.startswith(pattern + '/')]
                        if not matched:
                            raise FileNotFoundError(f"{pattern} not found in archive")
                        for m in matched:
                            tar.extract(m, path=dest_dir)
                        extracted_paths.append(os.path.join(dest_dir, pattern))

    elif archive_type == "zip":
        with zipfile.ZipFile(archive_path, "r") as zipf:
            namelist = zipf.namelist()

            for pattern in extract_list:
                if install_mode == 'single':
                    # Legacy behavior
                    matched = [z for z in namelist if os.path.basename(z) == pattern]
                    if not matched:
                        raise FileNotFoundError(f"{pattern} not found in archive")
                    for z in matched:
                        zipf.extract(z, path=dest_dir)
                    extracted_paths.append(os.path.join(dest_dir, matched[0]))

                elif install_mode == 'directory':
                    if pattern.endswith('/'):
                        # Extract directory contents (flatten)
                        dir_prefix = pattern.rstrip('/')
                        dir_files = [z for z in namelist
                                   if z.startswith(dir_prefix + '/') and z != dir_prefix and z != dir_prefix + '/']
                        if not dir_files:
                            raise FileNotFoundError(f"Directory {pattern} not found or empty in archive")

                        for z in dir_files:
                            zipf.extract(z, path=dest_dir)
                            rel_path = z[len(dir_prefix)+1:]
                            if rel_path and not z.endswith('/'):  # Skip directory entries
                                extracted_paths.append(os.path.join(dest_dir, z))
                    else:
                        matched = [z for z in namelist if z == pattern or z.startswith(pattern + '/')]
                        if not matched:
                            raise FileNotFoundError(f"{pattern} not found in archive")
                        for z in matched:
                            zipf.extract(z, path=dest_dir)
                        extracted_paths.append(os.path.join(dest_dir, pattern))
    else:
        raise ValueError(f"Unsupported archive type: {archive_type}")

    return extracted_paths


def resolve_yaml_source(yaml_path, recipe_repo=None):
    parsed = urlparse(yaml_path)
    if parsed.scheme in ("http", "https"):
        print(f"Fetching remote YAML: {yaml_path}")
        tmpfd, tmpfile = tempfile.mkstemp(suffix=".yaml")
        os.close(tmpfd)
        with urllib.request.urlopen(yaml_path) as response:
            with open(tmpfile, "wb") as out:
                out.write(response.read())
        return tmpfile, yaml_path

    if os.path.isfile(yaml_path):
        return yaml_path, os.path.abspath(yaml_path)

    if not recipe_repo:
        recipe_repo = get_recipe_repo()

    if recipe_repo:
        remote_url = get_raw_recipe_url(recipe_repo, yaml_path)
        print(f"Attempting to fetch recipe '{yaml_path}' from {remote_url}")
        tmpfd, tmpfile = tempfile.mkstemp(suffix=".yaml")
        os.close(tmpfd)
        try:
            with urllib.request.urlopen(remote_url) as response:
                with open(tmpfile, "wb") as out:
                    out.write(response.read())
            return tmpfile, remote_url
        except Exception as e:
            print(f"Failed to fetch recipe from remote: {e}")
            sys.exit(1)

    raise FileNotFoundError(
        f"YAML path not found and no valid recipe repo fallback: {yaml_path}"
    )


def install_tool(
    yaml_file, override_dest_dir=None, install_db_dir=None, recipe_repo=None
):
    local_yaml_file, source_yaml = resolve_yaml_source(
        yaml_file, recipe_repo=recipe_repo
    )

    with open(local_yaml_file) as f:
        spec = yaml.safe_load(f)

    name = spec["name"]
    version = spec.get("version", "unknown")
    chmod_spec = spec.get("chmod")
    install_mode = spec.get("install_mode", "single")  # NEW: Get install mode
    install_name = spec.get("install_name")  # NEW: Optional rename for installed directory
    symlink_bin = spec.get("symlink_bin")  # NEW: Path to create symlinks for bin/ executables
    source_yaml = os.path.abspath(yaml_file)
    context = collect_variables(spec)

    # Override top-level version with variable if available
    if "version" in context:
        version = context["version"]
    else:
        context["version"] = version

    url = evaluate_template(spec["url"], context) if "url" in spec else None
    dest_path = expand(evaluate_template(spec["dest"], context))

    # NEW: Evaluate install_name and symlink_bin with templating
    if install_name:
        install_name = evaluate_template(install_name, context)
    if symlink_bin:
        symlink_bin = expand(evaluate_template(symlink_bin, context))

    # NEW: Handle dest differently based on install_mode
    installed_files = []  # Track all installed files

    if install_mode == 'single':
        # Legacy behavior: dest is the full path to the file
        if override_dest_dir:
            override_dest_dir = expand(override_dest_dir)
            os.makedirs(override_dest_dir, exist_ok=True)
            final_dest = os.path.join(override_dest_dir, os.path.basename(dest_path))
        else:
            os.makedirs(os.path.dirname(dest_path), exist_ok=True)
            final_dest = dest_path
    elif install_mode == 'directory':
        # New behavior: dest is the parent directory for multiple files
        if override_dest_dir:
            final_dest = expand(override_dest_dir)
        else:
            final_dest = dest_path
        os.makedirs(final_dest, exist_ok=True)
    else:
        raise ValueError(f"Invalid install_mode: {install_mode}. Must be 'single' or 'directory'")

    if url:
        archive = spec.get("archive")
        extract = spec.get("extract")
        if extract is not None:
            extract = [evaluate_template(x, context) for x in extract]
        if archive and extract:
            with tempfile.TemporaryDirectory(prefix=f"pkg-{name}-") as tmpdir:
                tmp_archive = os.path.join(tmpdir, "archive")
                download(url, tmp_archive)
                # NEW: Pass install_mode and get list of extracted paths
                extracted_paths = extract_archive(archive, tmp_archive, extract, tmpdir, install_mode)

                if install_mode == 'single':
                    # Legacy: Copy single file
                    shutil.copy(extracted_paths[0], final_dest)
                    installed_files.append(final_dest)
                elif install_mode == 'directory':
                    # New: Copy each extracted file/directory to dest directory
                    for extracted_path in extracted_paths:
                        # Get the basename of the file or directory
                        basename = os.path.basename(extracted_path)

                        # NEW: Use install_name to rename if specified
                        if install_name and os.path.isdir(extracted_path):
                            dest_path = os.path.join(final_dest, install_name)
                        else:
                            dest_path = os.path.join(final_dest, basename)

                        if os.path.isdir(extracted_path):
                            # Copy entire directory tree
                            if os.path.exists(dest_path):
                                shutil.rmtree(dest_path)
                            shutil.copytree(extracted_path, dest_path)
                            installed_files.append(dest_path)
                            if install_name and install_name != basename:
                                print(f"Copied directory {basename} to {dest_path} (renamed to {install_name})")
                            else:
                                print(f"Copied directory {basename} to {dest_path}")
                        else:
                            # Copy single file
                            shutil.copy(extracted_path, dest_path)
                            installed_files.append(dest_path)
                            print(f"Copied {basename} to {dest_path}")
        else:
            # Direct download (no archive)
            download(url, final_dest)
            installed_files.append(final_dest)

    elif "git" in spec:
        git_url = evaluate_template(spec["git"], context)
        build_steps = spec.get("build", [])
        artifact = evaluate_template(spec.get("artifact", ""), context)
        if not artifact:
            raise ValueError("Missing 'artifact' field for git build")
        with tempfile.TemporaryDirectory(prefix=f"pkg-{name}-") as tmpdir:
            print(f"Cloning {git_url} into {tmpdir}")
            subprocess.run(["git", "clone", "--depth=1", git_url, tmpdir], check=True)
            run_build_steps(build_steps, cwd=tmpdir)
            artifact_path = os.path.join(tmpdir, artifact)
            if not os.path.exists(artifact_path):
                raise FileNotFoundError(f"Built artifact not found: {artifact_path}")
            # Git builds currently only support single file mode
            if install_mode == 'single':
                shutil.copy(artifact_path, final_dest)
                installed_files.append(final_dest)
            else:
                raise ValueError("Git builds currently only support install_mode='single'")

    else:
        raise ValueError("YAML must contain either 'url' or 'git' field")

    # NEW: Apply chmod using new helper function
    if install_mode == 'single':
        apply_chmod_recursive(final_dest, chmod_spec, install_mode)
    else:
        # For directory mode, apply chmod to each installed file
        for installed_file in installed_files:
            apply_chmod_recursive(installed_file, chmod_spec, install_mode)

    # NEW: Create symlinks for bin executables if requested
    created_symlinks = []
    if symlink_bin and install_mode == 'directory':
        os.makedirs(symlink_bin, exist_ok=True)
        # Find all executables in installed_files/bin/ directory
        for installed_path in installed_files:
            bin_dir = os.path.join(installed_path, 'bin')
            if os.path.isdir(bin_dir):
                for fname in os.listdir(bin_dir):
                    fpath = os.path.join(bin_dir, fname)
                    if os.path.isfile(fpath) and os.access(fpath, os.X_OK):
                        # Create symlink in symlink_bin directory
                        symlink_path = os.path.join(symlink_bin, fname)
                        if os.path.lexists(symlink_path):
                            os.remove(symlink_path)  # Remove existing symlink/file
                        os.symlink(fpath, symlink_path)
                        created_symlinks.append(symlink_path)
                        print(f"Created symlink: {symlink_path} -> {fpath}")

    # Print installation summary
    if install_mode == 'single':
        print(f"Installed {name} to {final_dest}")
    else:
        print(f"Installed {name} to {final_dest} ({len(installed_files)} files)")
        for f in installed_files:
            print(f"  - {f}")
        if created_symlinks:
            print(f"Created {len(created_symlinks)} symlink(s) in {symlink_bin}")

    install_db_dir = get_install_db_path(install_db_dir)
    os.makedirs(install_db_dir, exist_ok=True)

    hash_input = f"{name}|{version}|{final_dest}|{source_yaml}"
    install_id = hashlib.sha256(hash_input.encode()).hexdigest()[:12]
    filename = f"PKG_{install_id}.yaml"

    # NEW: Calculate SHA256 checksums for all installed files
    checksums = {}
    all_files = []  # Track all individual files for checksums
    for filepath in installed_files:
        if os.path.isfile(filepath):
            checksum = calculate_sha256(filepath)
            if checksum:
                checksums[filepath] = checksum
                all_files.append(filepath)
        elif os.path.isdir(filepath):
            # For directories, calculate checksums for all files within
            for root, dirs, files in os.walk(filepath):
                for fname in files:
                    fpath = os.path.join(root, fname)
                    checksum = calculate_sha256(fpath)
                    if checksum:
                        checksums[fpath] = checksum
                        all_files.append(fpath)

    # NEW: Enhanced metadata format with install_mode and installed_files
    rendered_metadata = {
        "name": name,
        "version": version,
        "url": url,
        "dest": dest_path,
        "installed_to": final_dest,
        "installed_at": datetime.now(timezone.utc).isoformat(),
        "variables": context,
        "install_mode": install_mode,  # NEW: Track install mode
        "installed_files": installed_files,  # NEW: Track all installed files
        "checksums": checksums,  # NEW: Track SHA256 checksums
        "symlinks": created_symlinks,  # NEW: Track created symlinks
    }

    with open(os.path.join(install_db_dir, filename), "w") as f:
        yaml.safe_dump_all([spec, rendered_metadata], f, sort_keys=False)


def list_installed(install_db_dir=None):
    install_db_dir = get_install_db_path(install_db_dir)
    if not os.path.isdir(install_db_dir):
        print("No tools installed.")
        return

    # NEW: Updated header with MODE and INSTALLED columns
    print(f"{'ID':12} {'NAME':20} {'VERSION':10} {'MODE':8} {'INSTALLED':16} {'PATH'}")
    for fname in sorted(os.listdir(install_db_dir)):
        if not fname.startswith("PKG_") or not fname.endswith(".yaml"):
            continue
        install_id = fname[4:-5]
        with open(os.path.join(install_db_dir, fname)) as f:
            # Optimization: Skip loading the entire file with checksums
            # Read file content and split on document separator
            content = f.read()
            # Find the second YAML document (metadata)
            parts = content.split('\n---\n', 1)
            if len(parts) < 2:
                continue
            # Parse only the metadata portion, skip checksums to avoid loading 14k+ entries
            metadata_yaml = parts[1]
            # Find where checksums section starts and truncate there
            checksum_pos = metadata_yaml.find('\nchecksums:')
            if checksum_pos != -1:
                # Only parse metadata up to checksums
                metadata_yaml = metadata_yaml[:checksum_pos]
            meta = yaml.safe_load(metadata_yaml) or {}

            # NEW: Get install_mode and file count
            install_mode = meta.get('install_mode', 'single')
            mode_display = install_mode[:6]  # Truncate for display
            installed_path = meta.get('installed_to', '')

            # NEW: Format installed_at timestamp
            installed_at = meta.get('installed_at', '')
            if installed_at:
                try:
                    # Parse ISO format timestamp and show just date and time
                    dt = datetime.fromisoformat(installed_at.replace('Z', '+00:00'))
                    date_display = dt.strftime('%Y-%m-%dT%H:%M')
                except:
                    date_display = installed_at[:16]  # Fallback to first 16 chars
            else:
                date_display = 'unknown'

            # NEW: Show count for multi-file installations
            if install_mode != 'single' and 'installed_files' in meta:
                file_count = len(meta['installed_files'])
                installed_path = f"{installed_path} ({file_count} files)"

            print(
                f"{install_id:12} {meta.get('name',''):20} {meta.get('version',''):10} {mode_display:8} {date_display:16} {installed_path}"
            )


def uninstall_tool(install_id, install_db_dir=None):
    install_db_dir = get_install_db_path(install_db_dir)
    fname = f"PKG_{install_id}.yaml"
    path = os.path.join(install_db_dir, fname)
    if not os.path.exists(path):
        print(f"No metadata found for install ID {install_id}")
        return

    with open(path) as f:
        docs = list(yaml.safe_load_all(f))
        meta = docs[1] if len(docs) > 1 else {}

    # NEW: Get install_mode and installed_files with backward compatibility
    install_mode = meta.get("install_mode", "single")
    installed_files = meta.get("installed_files", [])
    symlinks = meta.get("symlinks", [])

    # Backward compatibility: fall back to installed_to if no installed_files
    if not installed_files:
        installed_to = meta.get("installed_to")
        if installed_to:
            installed_files = [installed_to]

    # NEW: Remove symlinks first
    if symlinks:
        for symlink in symlinks:
            if os.path.lexists(symlink):
                print(f"Removing symlink: {symlink}")
                try:
                    os.remove(symlink)
                except Exception as e:
                    print(f"Failed to remove symlink {symlink}: {e}")

    # Remove all installed files/directories
    removed_count = 0
    for item in installed_files:
        if not item:
            continue

        if os.path.isdir(item):
            # Remove directory tree
            print(f"Removing directory: {item}")
            try:
                shutil.rmtree(item)
                removed_count += 1
            except Exception as e:
                print(f"Failed to remove directory {item}: {e}")
        elif os.path.isfile(item):
            # Remove single file
            print(f"Removing file: {item}")
            try:
                os.remove(item)
                removed_count += 1
            except Exception as e:
                print(f"Failed to remove file {item}: {e}")
        else:
            print(f"Path does not exist (may have been manually deleted): {item}")

    if removed_count > 0:
        print(f"Removed {removed_count} file(s)/directory(ies)")

    print(f"Removing metadata: {path}")
    os.remove(path)


def verify_tool(install_id, install_db_dir=None):
    """Verify that installed files exist and match their stored checksums."""
    install_db_dir = get_install_db_path(install_db_dir)
    fname = f"PKG_{install_id}.yaml"
    path = os.path.join(install_db_dir, fname)

    if not os.path.exists(path):
        print(f"No metadata found for install ID {install_id}")
        return False

    with open(path) as f:
        docs = list(yaml.safe_load_all(f))
        meta = docs[1] if len(docs) > 1 else {}

    name = meta.get("name", "unknown")
    version = meta.get("version", "unknown")
    install_mode = meta.get("install_mode", "single")
    installed_files = meta.get("installed_files", [])
    stored_checksums = meta.get("checksums", {})

    # Backward compatibility: fall back to installed_to if no installed_files
    if not installed_files:
        installed_to = meta.get("installed_to")
        if installed_to:
            installed_files = [installed_to]

    if not installed_files:
        print(f"✗ {name} {version}: No files tracked in metadata")
        return False

    print(f"Verifying {name} {version} ({install_mode} mode):")

    all_valid = True

    # If we have checksums, verify all files with checksums
    if stored_checksums:
        valid_count = 0
        invalid_count = 0
        missing_count = 0

        for filepath, expected_checksum in stored_checksums.items():
            if not os.path.exists(filepath):
                print(f"  ✗ {filepath}: File missing")
                missing_count += 1
                all_valid = False
            else:
                actual_checksum = calculate_sha256(filepath)
                if actual_checksum == expected_checksum:
                    valid_count += 1
                else:
                    print(f"  ✗ {filepath}: Checksum mismatch")
                    print(f"    Expected: {expected_checksum}")
                    print(f"    Actual:   {actual_checksum}")
                    invalid_count += 1
                    all_valid = False

        # Show summary
        total = len(stored_checksums)
        print(f"  Summary: {valid_count}/{total} files valid", end="")
        if invalid_count > 0:
            print(f", {invalid_count} checksum mismatches", end="")
        if missing_count > 0:
            print(f", {missing_count} missing", end="")
        print()
    else:
        # No checksums stored (old metadata format) - just check existence
        for filepath in installed_files:
            if not os.path.exists(filepath):
                print(f"  ✗ {filepath}: Path missing")
                all_valid = False
            elif os.path.isfile(filepath):
                print(f"  ? {filepath}: Exists (no checksum to verify)")
            elif os.path.isdir(filepath):
                print(f"  ? {filepath}: Directory exists (no checksum)")

    return all_valid


def get_github_token():
    """
    Get GitHub token from environment variable or .github_token file.

    Returns token string or None if not found.
    """
    # Try environment variable first
    token = os.environ.get("GITHUB_TOKEN")
    if token:
        return token

    # Fallback to .github_token file
    token_file = os.path.join(os.path.dirname(__file__), ".github_token")
    if os.path.isfile(token_file):
        try:
            with open(token_file, "r") as f:
                token = f.read().strip()
                if token:
                    return token
        except Exception:
            pass

    return None


def check_update(name, current_version, spec):
    """
    Check for available updates for a package.

    Returns (latest_version, source) or (None, error_message)
    """
    # Look for update script in updates/ directory
    script_dir = os.path.join(os.path.dirname(__file__), "updates")

    # Get GitHub token for authenticated API calls
    github_token = get_github_token()

    # Try multiple extensions: .sh, .py, or no extension
    for script_name in [f"{name}.sh", f"{name}.py", name]:
        script_path = os.path.join(script_dir, script_name)
        if os.path.isfile(script_path) and os.access(script_path, os.X_OK):
            try:
                # Pass GitHub token as environment variable
                env = os.environ.copy()
                if github_token:
                    env["GITHUB_TOKEN"] = github_token

                result = subprocess.run(
                    [script_path],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    timeout=30,
                    text=True,
                    env=env
                )
                if result.returncode == 0:
                    latest_version = result.stdout.strip()
                    if latest_version:
                        return latest_version, f"updates/{script_name}"
                else:
                    return None, f"Script failed: {result.stderr.strip()}"
            except subprocess.TimeoutExpired:
                return None, "Script timeout"
            except Exception as e:
                return None, f"Script error: {str(e)}"

    # Fallback: Auto-detect from GitHub URL or git repo
    github_url = spec.get("url", "") or spec.get("git", "")

    if "github.com" in github_url:
        # Extract owner/repo from URL
        # Example: https://github.com/stern/stern/releases/download/v1.32.0/...
        # Or: https://github.com/openshift/oc
        parts = github_url.split("github.com/")
        if len(parts) > 1:
            repo_parts = parts[1].split("/")
            if len(repo_parts) >= 2:
                owner, repo = repo_parts[0], repo_parts[1]
                # Remove .git suffix if present
                repo = repo.rstrip(".git")

                try:
                    import json

                    # Build curl command with authentication if token is available
                    curl_base = ["curl", "-s"]
                    if github_token:
                        curl_base.extend(["-H", f"Authorization: token {github_token}"])

                    # Try releases first
                    api_url = f"https://api.github.com/repos/{owner}/{repo}/releases/latest"
                    result = subprocess.run(
                        curl_base + [api_url],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        timeout=10,
                        text=True
                    )
                    if result.returncode == 0:
                        try:
                            data = json.loads(result.stdout)
                            tag = data.get("tag_name", "")
                            if tag:
                                # Strip common prefixes
                                latest = tag.lstrip("v").lstrip("V")
                                return latest, "github-releases"
                        except json.JSONDecodeError:
                            pass

                    # Fallback: Try tags if no releases
                    api_url = f"https://api.github.com/repos/{owner}/{repo}/tags"
                    result = subprocess.run(
                        curl_base + [api_url],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        timeout=10,
                        text=True
                    )
                    if result.returncode == 0:
                        try:
                            data = json.loads(result.stdout)
                            if data and len(data) > 0:
                                tag = data[0].get("name", "")
                                if tag:
                                    # Strip common prefixes
                                    latest = tag.lstrip("v").lstrip("V")
                                    return latest, "github-tags"
                        except json.JSONDecodeError:
                            pass
                except:
                    pass

    return None, "No update check available"


def check_all_updates(install_db_dir=None):
    """Check for updates for all installed packages."""
    install_db_dir = get_install_db_path(install_db_dir)
    if not os.path.isdir(install_db_dir):
        print("No tools installed.")
        return

    print(f"{'NAME':20} {'CURRENT':15} {'LATEST':15} {'STATUS':12} {'SOURCE':20}")
    print("=" * 82)

    update_count = 0

    for fname in sorted(os.listdir(install_db_dir)):
        if not fname.startswith("PKG_") or not fname.endswith(".yaml"):
            continue

        with open(os.path.join(install_db_dir, fname)) as f:
            # Read only the first document (recipe spec)
            content = f.read()
            parts = content.split('\n---\n', 1)
            if len(parts) < 1:
                continue

            spec = yaml.safe_load(parts[0])
            if not spec:
                continue

            name = spec.get("name", "")
            current_version = spec.get("version", "unknown")

            # Evaluate variables for templating (needed for git URLs)
            context = collect_variables(spec)
            if "version" not in context:
                context["version"] = current_version

            # Evaluate git URL if it's templated
            if "git" in spec:
                git_url = spec["git"]
                if "{{" in git_url:
                    spec["git"] = evaluate_template(git_url, context)

            # Check for updates
            latest_version, source = check_update(name, current_version, spec)

            if latest_version:
                if latest_version != current_version:
                    status = "UPDATE"
                    update_count += 1
                else:
                    status = "up-to-date"
            else:
                status = "n/a"
                latest_version = "-"

            print(f"{name:20} {current_version:15} {latest_version:15} {status:12} {source:20}")

    print("=" * 82)
    if update_count > 0:
        print(f"{update_count} package(s) have updates available")
    else:
        print("All packages are up to date")


def update_recipe(name, new_version, recipes_dir=None):
    """
    Update the version field in a recipe YAML file.

    Args:
        name: Recipe name (e.g., 'helm', 'kubectl')
        new_version: New version string to set
        recipes_dir: Path to recipes directory (defaults to ./recipes relative to script)

    Returns:
        bool: True if successful, False otherwise
    """
    # Determine recipes directory
    if recipes_dir is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        recipes_dir = os.path.join(script_dir, "recipes")

    recipe_path = os.path.join(recipes_dir, f"{name}.yaml")

    # Check if recipe file exists
    if not os.path.isfile(recipe_path):
        print(f"Error: Recipe file not found: {recipe_path}")
        return False

    # Check if file is writable
    if not os.access(recipe_path, os.W_OK):
        print(f"Error: Recipe file is not writable: {recipe_path}")
        return False

    try:
        # Read the file line by line to preserve formatting
        with open(recipe_path, 'r') as f:
            lines = f.readlines()

        # Find and update the version line
        version_updated = False
        for i, line in enumerate(lines):
            # Match version field (typically line 2, index 1)
            if line.strip().startswith('version:'):
                # Extract indentation and update value
                indent = len(line) - len(line.lstrip())
                lines[i] = ' ' * indent + f'version: {new_version}\n'
                version_updated = True
                break

        if not version_updated:
            print(f"Error: Could not find 'version:' field in {recipe_path}")
            return False

        # Write atomically using temp file
        temp_path = recipe_path + '.tmp'
        with open(temp_path, 'w') as f:
            f.writelines(lines)

        # Atomic rename
        os.rename(temp_path, recipe_path)

        return True

    except PermissionError as e:
        print(f"Error: Permission denied updating {recipe_path}: {e}")
        return False
    except IOError as e:
        print(f"Error: I/O error updating {recipe_path}: {e}")
        # Clean up temp file if it exists
        temp_path = recipe_path + '.tmp'
        if os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass
        return False
    except Exception as e:
        print(f"Error: Unexpected error updating {recipe_path}: {e}")
        return False


def update_command(package_names=None, update_all=False, skip_confirmation=False,
                   install_db_dir=None, recipes_dir=None):
    """
    Update recipe files with latest versions.

    Args:
        package_names: List of package names to update (or None if update_all=True)
        update_all: Update all installed packages
        skip_confirmation: Skip confirmation prompt (--yes flag)
        install_db_dir: Path to install database
        recipes_dir: Path to recipes directory
    """
    install_db_dir = get_install_db_path(install_db_dir)

    if recipes_dir is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        recipes_dir = os.path.join(script_dir, "recipes")

    # Phase 1: Gather candidates
    candidates = []

    if update_all:
        # Get all installed packages
        if not os.path.isdir(install_db_dir):
            print("No tools installed.")
            return

        for fname in sorted(os.listdir(install_db_dir)):
            if not fname.startswith("PKG_") or not fname.endswith(".yaml"):
                continue

            with open(os.path.join(install_db_dir, fname)) as f:
                content = f.read()
                parts = content.split('\n---\n', 1)
                if len(parts) < 1:
                    continue
                spec = yaml.safe_load(parts[0])
                if not spec:
                    continue

                name = spec.get("name", "")
                current_version = spec.get("version", "unknown")
                candidates.append((name, current_version, spec))
    else:
        # Get specific packages
        if not os.path.isdir(install_db_dir):
            print("No tools installed.")
            return

        for name in package_names:
            # Find package in install DB
            found = False
            for fname in os.listdir(install_db_dir):
                if not fname.startswith("PKG_") or not fname.endswith(".yaml"):
                    continue

                with open(os.path.join(install_db_dir, fname)) as f:
                    content = f.read()
                    parts = content.split('\n---\n', 1)
                    if len(parts) < 1:
                        continue
                    spec = yaml.safe_load(parts[0])
                    if not spec or spec.get("name") != name:
                        continue

                    current_version = spec.get("version", "unknown")
                    candidates.append((name, current_version, spec))
                    found = True
                    break

            if not found:
                print(f"Warning: Package '{name}' is not installed")

    if not candidates:
        print("No packages to check for updates")
        return

    # Phase 2: Check for updates
    updates = []

    for name, current_version, spec in candidates:
        # Evaluate variables for templating
        context = collect_variables(spec)
        if "version" not in context:
            context["version"] = current_version

        # Evaluate git URL if it's templated
        if "git" in spec:
            git_url = spec["git"]
            if "{{" in git_url:
                spec["git"] = evaluate_template(git_url, context)

        # Check for update
        latest_version, source = check_update(name, current_version, spec)

        if latest_version and latest_version != current_version:
            # Verify recipe file exists locally
            recipe_path = os.path.join(recipes_dir, f"{name}.yaml")
            if not os.path.isfile(recipe_path):
                print(f"Warning: Recipe file not found for '{name}': {recipe_path}")
                print(f"         Package may have been installed from remote recipe")
                continue

            updates.append({
                'name': name,
                'current_version': current_version,
                'latest_version': latest_version,
                'source': source
            })

    if not updates:
        print("All recipes are already up to date")
        return

    # Phase 3: Display and confirm
    print("\nThe following recipes will be updated:\n")
    print(f"{'NAME':20} {'CURRENT':15} {'LATEST':15} {'SOURCE':20}")
    print("=" * 70)

    for update in updates:
        print(f"{update['name']:20} {update['current_version']:15} "
              f"{update['latest_version']:15} {update['source']:20}")

    print("=" * 70)
    print(f"{len(updates)} recipe(s) will be updated\n")

    if not skip_confirmation:
        try:
            response = input("Proceed with updates? [y/N]: ")
            if response.lower() not in ('y', 'yes'):
                print("Update cancelled")
                return
        except (KeyboardInterrupt, EOFError):
            print("\nUpdate cancelled")
            return

    # Phase 4: Execute updates
    print("\nUpdating recipes...")
    success_count = 0
    failed_count = 0

    for update in updates:
        name = update['name']
        new_version = update['latest_version']

        if update_recipe(name, new_version, recipes_dir):
            print(f"✓ Updated {name}: {update['current_version']} -> {new_version}")
            success_count += 1
        else:
            print(f"✗ Failed to update {name}")
            failed_count += 1

    # Summary
    print(f"\nSummary: {success_count} updated", end="")
    if failed_count > 0:
        print(f", {failed_count} failed")
    else:
        print()


def update_packages(package_names=None, update_all=False, skip_confirmation=False,
                    install_db_dir=None, override_dest_dir=None, recipe_repo=None):
    """
    Update installed packages to match their recipe versions.

    Args:
        package_names: List of package names to update (or None if update_all=True)
        update_all: Update all installed packages
        skip_confirmation: Skip confirmation prompt (--yes flag)
        install_db_dir: Path to install database
        override_dest_dir: Override destination directory for installation
        recipe_repo: Override recipe repository URL
    """
    install_db_dir = get_install_db_path(install_db_dir)

    if not os.path.isdir(install_db_dir):
        print("No tools installed.")
        return

    # Phase 1: Gather candidates
    candidates = []

    if update_all:
        # Get all installed packages
        for fname in sorted(os.listdir(install_db_dir)):
            if not fname.startswith("PKG_") or not fname.endswith(".yaml"):
                continue

            install_id = fname.replace("PKG_", "").replace(".yaml", "")
            with open(os.path.join(install_db_dir, fname)) as f:
                content = f.read()
                parts = content.split('\n---\n', 1)
                if len(parts) < 1:
                    continue
                spec = yaml.safe_load(parts[0])
                if not spec:
                    continue

                name = spec.get("name", "")
                current_version = spec.get("version", "unknown")
                # Store the original recipe source (file path or name for remote lookup)
                recipe_source = spec.get("_recipe_source", name)
                candidates.append((name, current_version, recipe_source, install_id))
    else:
        # Get specific packages
        for name in package_names:
            # Find package in install DB
            found = False
            for fname in os.listdir(install_db_dir):
                if not fname.startswith("PKG_") or not fname.endswith(".yaml"):
                    continue

                install_id = fname.replace("PKG_", "").replace(".yaml", "")
                with open(os.path.join(install_db_dir, fname)) as f:
                    content = f.read()
                    parts = content.split('\n---\n', 1)
                    if len(parts) < 1:
                        continue
                    spec = yaml.safe_load(parts[0])
                    if not spec or spec.get("name") != name:
                        continue

                    current_version = spec.get("version", "unknown")
                    recipe_source = spec.get("_recipe_source", name)
                    candidates.append((name, current_version, recipe_source, install_id))
                    found = True
                    break

            if not found:
                print(f"Warning: Package '{name}' is not installed")

    if not candidates:
        print("No packages to check for updates")
        return

    # Phase 2: Check recipe versions
    updates = []

    for name, current_version, recipe_source, install_id in candidates:
        try:
            # Load the recipe to get the version
            # Try local file first, then remote
            script_dir = os.path.dirname(os.path.abspath(__file__))
            local_recipe_path = os.path.join(script_dir, "recipes", f"{name}.yaml")

            recipe_content = None
            if os.path.isfile(local_recipe_path):
                with open(local_recipe_path) as f:
                    recipe_content = f.read()
            else:
                # Try remote recipe
                try:
                    recipe_url = get_raw_recipe_url(name, recipe_repo)
                    result = subprocess.run(
                        ["curl", "-s", "-f", recipe_url],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        timeout=10,
                        text=True
                    )
                    if result.returncode == 0:
                        recipe_content = result.stdout
                except:
                    pass

            if not recipe_content:
                print(f"Warning: Could not load recipe for '{name}'")
                continue

            # Parse recipe version
            recipe_spec = yaml.safe_load(recipe_content)
            recipe_version = recipe_spec.get("version", "unknown")

            # Compare versions
            if recipe_version != current_version:
                updates.append({
                    'name': name,
                    'current_version': current_version,
                    'recipe_version': recipe_version,
                    'recipe_source': recipe_source,
                    'install_id': install_id
                })

        except Exception as e:
            print(f"Warning: Error checking recipe for '{name}': {e}")
            continue

    if not updates:
        print("All packages are up to date with their recipes")
        return

    # Phase 3: Display and confirm
    print("\nThe following packages will be updated:\n")
    print(f"{'NAME':20} {'CURRENT':15} {'RECIPE':15}")
    print("=" * 50)

    for update in updates:
        print(f"{update['name']:20} {update['current_version']:15} "
              f"{update['recipe_version']:15}")

    print("=" * 50)
    print(f"{len(updates)} package(s) will be updated\n")

    if not skip_confirmation:
        try:
            response = input("Proceed with updates? [y/N]: ")
            if response.lower() not in ('y', 'yes'):
                print("Update cancelled")
                return
        except (KeyboardInterrupt, EOFError):
            print("\nUpdate cancelled")
            return

    # Phase 4: Execute updates
    print("\nUpdating packages...")
    success_count = 0
    failed_count = 0

    for update in updates:
        name = update['name']

        print(f"\nUpdating {name} ({update['current_version']} -> {update['recipe_version']})...")
        try:
            # Uninstall current version first
            uninstall_tool(update['install_id'], install_db_dir)

            # Install new version from recipe
            # Use local recipe if it exists, otherwise use the original recipe source
            script_dir = os.path.dirname(os.path.abspath(__file__))
            local_recipe_path = os.path.join(script_dir, "recipes", f"{name}.yaml")

            if os.path.isfile(local_recipe_path):
                recipe_to_use = local_recipe_path
            else:
                recipe_to_use = update['recipe_source']

            install_tool(
                recipe_to_use,
                override_dest_dir=override_dest_dir,
                install_db_dir=install_db_dir,
                recipe_repo=recipe_repo
            )
            print(f"✓ Updated {name} to {update['recipe_version']}")
            success_count += 1
        except Exception as e:
            print(f"✗ Failed to update {name}: {e}")
            failed_count += 1

    # Summary
    print(f"\nSummary: {success_count} updated", end="")
    if failed_count > 0:
        print(f", {failed_count} failed")
    else:
        print()


def verify_all_tools(install_db_dir=None):
    """Verify all installed packages."""
    install_db_dir = get_install_db_path(install_db_dir)
    if not os.path.isdir(install_db_dir):
        print("No tools installed.")
        return True

    all_packages_valid = True
    package_count = 0
    valid_count = 0
    invalid_count = 0

    for fname in sorted(os.listdir(install_db_dir)):
        if not fname.startswith("PKG_") or not fname.endswith(".yaml"):
            continue

        install_id = fname[4:-5]
        package_count += 1

        # Verify this package
        is_valid = verify_tool(install_id, install_db_dir)
        if is_valid:
            valid_count += 1
        else:
            invalid_count += 1
            all_packages_valid = False

        print()  # Blank line between packages

    # Print summary
    print("=" * 60)
    print(f"Verification Summary:")
    print(f"  Total packages: {package_count}")
    print(f"  Valid: {valid_count}")
    print(f"  Invalid: {invalid_count}")

    return all_packages_valid


def main():
    parser = argparse.ArgumentParser(description="Minimal YAML-based installer (pkg)")
    subparsers = parser.add_subparsers(dest="command", required=True)

    def add_common_args(p):
        p.add_argument("--dest-dir", help="Override destination directory (e.g. ~/bin)")
        p.add_argument("--install-db", help="Override install DB path (or use $PKG_DB)")
        p.add_argument(
            "--recipe-repo", help="Recipe repo (e.g. GitHub URL or local path)"
        )
        p.add_argument("yaml_file", help="YAML file, URL, or recipe name")

    p_install = subparsers.add_parser("install", help="Install a tool from YAML")
    add_common_args(p_install)

    p_list = subparsers.add_parser("list", help="List installed tools")
    p_list.add_argument("--install-db", help="Override install DB path")

    p_uninstall = subparsers.add_parser(
        "uninstall", help="Uninstall a tool by install ID"
    )
    p_uninstall.add_argument("install_id", help="Install ID shown by 'pkg list'")
    p_uninstall.add_argument("--install-db", help="Override install DB path")

    p_verify = subparsers.add_parser(
        "verify", help="Verify installed files match their checksums"
    )
    p_verify.add_argument("install_id", nargs='?', help="Install ID shown by 'pkg list' (omit to verify all)")
    p_verify.add_argument("--install-db", help="Override install DB path")
    p_verify.add_argument("--all", action="store_true", help="Verify all installed packages")

    p_check_updates = subparsers.add_parser(
        "check-updates", help="Check for available updates"
    )
    p_check_updates.add_argument("--install-db", help="Override install DB path")

    p_update_recipes = subparsers.add_parser(
        "update-recipes", help="Update recipe version numbers"
    )
    p_update_recipes.add_argument(
        "package_name", nargs='*', help="Package name(s) to update"
    )
    p_update_recipes.add_argument(
        "--all", action="store_true", help="Update all installed packages"
    )
    p_update_recipes.add_argument(
        "--yes", "-y", action="store_true", help="Skip confirmation prompt"
    )
    p_update_recipes.add_argument("--install-db", help="Override install DB path")
    p_update_recipes.add_argument("--recipes-dir", help="Override recipes directory path")

    p_update = subparsers.add_parser(
        "update", help="Update installed packages to match recipe versions"
    )
    p_update.add_argument(
        "package_name", nargs='*', help="Package name(s) to update"
    )
    p_update.add_argument(
        "--all", action="store_true", help="Update all installed packages"
    )
    p_update.add_argument(
        "--yes", "-y", action="store_true", help="Skip confirmation prompt"
    )
    p_update.add_argument("--install-db", help="Override install DB path")
    p_update.add_argument("--dest-dir", help="Override destination directory")
    p_update.add_argument("--recipe-repo", help="Override recipe repository URL")

    args = parser.parse_args()

    if args.command == "install":
        install_tool(
            args.yaml_file,
            override_dest_dir=args.dest_dir,
            install_db_dir=args.install_db,
            recipe_repo=args.recipe_repo,
        )
    elif args.command == "list":
        list_installed(args.install_db)
    elif args.command == "uninstall":
        uninstall_tool(args.install_id, args.install_db)
    elif args.command == "verify":
        if args.all or args.install_id is None:
            verify_all_tools(args.install_db)
        else:
            verify_tool(args.install_id, args.install_db)
    elif args.command == "check-updates":
        check_all_updates(args.install_db)
    elif args.command == "update-recipes":
        if not args.all and not args.package_name:
            print("Error: Specify package name(s) or use --all")
            sys.exit(1)

        update_command(
            package_names=args.package_name if args.package_name else None,
            update_all=args.all,
            skip_confirmation=args.yes,
            install_db_dir=args.install_db,
            recipes_dir=getattr(args, 'recipes_dir', None)
        )
    elif args.command == "update":
        if not args.all and not args.package_name:
            print("Error: Specify package name(s) or use --all")
            sys.exit(1)

        update_packages(
            package_names=args.package_name if args.package_name else None,
            update_all=args.all,
            skip_confirmation=args.yes,
            install_db_dir=args.install_db,
            override_dest_dir=getattr(args, 'dest_dir', None),
            recipe_repo=getattr(args, 'recipe_repo', None)
        )


if __name__ == "__main__":
    main()
